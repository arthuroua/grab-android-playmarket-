import os
import re
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from pathlib import Path

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
start_url = "https://web.archive.org/web/20240907100524/http://android-playmarket.com/"
site_dir = Path("android-playmarket-site")
site_dir.mkdir(exist_ok=True)

# –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ HTML
r = requests.get(start_url, timeout=20)
soup = BeautifulSoup(r.text, "html.parser")

# –°–ø–∏—Å–æ–∫ —Ä–æ–∑—à–∏—Ä–µ–Ω—å, —è–∫—ñ –æ–±—Ä–æ–±–ª—è—î–º–æ
extensions = [".css", ".js", ".png", ".jpg", ".jpeg", ".gif", ".ico", ".woff", ".woff2", ".ttf", ".eot"]

def clean_filename(url: str) -> Path:
    """
    –ì–µ–Ω–µ—Ä—É—î –∫–æ—Ä–µ–∫—Ç–Ω–∏–π —à–ª—è—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—É –∑ URL,
    –≤–∏—Ä—ñ–∑–∞—é—á–∏ –ø—Ä–µ—Ñ—ñ–∫—Å Wayback Machine —ñ –∑–∞–º—ñ–Ω—é—é—á–∏ –≤—Å—ñ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω—ñ —Å–∏–º–≤–æ–ª–∏.
    """
    m = re.match(r"https?://web\.archive\.org/web/\d+[a-z_]*?/(https?://.+)", url)
    if m:
        url = m.group(1)

    parsed = urlparse(url)
    # –°—Ç–≤–æ—Ä—é—î–º–æ –±–∞–∑–æ–≤–∏–π —à–ª—è—Ö –±–µ–∑ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–∏—Ö —Å–∏–º–≤–æ–ª—ñ–≤
    clean_path = parsed.netloc + parsed.path
    clean_path = re.sub(r"[<>:\"/\\|?*]", "_", clean_path)

    # –î–æ–¥–∞—î–º–æ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è, —è–∫—â–æ –π–æ–≥–æ –Ω–µ–º–∞
    if not os.path.splitext(clean_path)[1]:
        clean_path += ".html"

    return site_dir / clean_path

def download(url):
    filename = clean_filename(url)
    try:
        if not filename.parent.exists():
            filename.parent.mkdir(parents=True, exist_ok=True)

        if not filename.exists():
            r = requests.get(url, timeout=10)
            with open(filename, "wb") as f:
                f.write(r.content)
            print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {url}")
    except Exception as e:
        print(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ {url}: {e}")

def rewrite_links(soup, tag, attr):
    for el in soup.find_all(tag):
        link = el.get(attr)
        if not link or not any(ext in link for ext in extensions):
            continue
        full = urljoin(start_url, link)
        download(full)
        local_path = clean_filename(full).relative_to(site_dir).as_posix()
        el[attr] = local_path

# –û–±—Ä–æ–±–∫–∞ CSS, JS, –∑–æ–±—Ä–∞–∂–µ–Ω—å
rewrite_links(soup, "link", "href")
rewrite_links(soup, "script", "src")
rewrite_links(soup, "img", "src")

# –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è HTML
main_file = site_dir / "index.html"
with open(main_file, "w", encoding="utf-8") as f:
    f.write(str(soup))

print("üìÅ –°–∞–π—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤:", main_file)
